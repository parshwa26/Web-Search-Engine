https://towardsdatascience.com/increase-your-face-recognition-models-accuracy-by-improving-face-contrast-a3e71bb6b9fb?source=collection_category---4------4-----------------------
Increase Your Face Recognition Model’s Accuracy by Improving Face Contrast Sign in Data Science Machine Learning Programming Visualization AI Video About Contribute Increase Your Face Recognition Model’s Accuracy by Improving Face Contrast Pre-processing images for better results Dieter Jordens Follow Mar 22 · 7 min read Photo by Tiko Giorgadze on Unsplash A Contrast Improving Image Preprocessing Step Let me introduce you to a problem I had a while ago. I was experimenting with face detection and OpenCV to explore the latest machine and deep learning models. I noticed that for faces, which have a clear contrast, the detection and recognition rate was significantly higher. That got me thinking, how can I improve the contrast for images automatically? I will show you what histogram equalization is. At the end of this blog post, you will have learned a preprocessing step that has the ability to improve your recognition rate. I’ll start by explaining what contrast really means in image processing and we’ll work our way up from there. In the end, I will show you how to apply a very advanced histogram equalization method on colored images, the so-called Contrast Limited Adaptive Histogram Equalization algorithm (CLAHE). What Is Contrast in an Image? “The difference in color or luminance is what makes an object distinguishable from other objects within the same field of view.” — Wikipedia This definition makes sense to everyone who is familiar with photography. It basically tells us directly what we need in order to separate objects from their background. What we do not need is low or high contrast, but something in between. It needs to be optimal. Difference between high and low contrast images (Source: filmschoolonline.com). The image on the left has a lower contrast than the image on the right. It is more difficult to clearly see the contours of the left face. While the latter has better contours, an image with too much contrast loses subtle information. This last part could be something my inner data scientist would worry about. When you improve the contrast before classifying an object, you are basically removing a bit of noise and it tends to improve the model. The big contours are the most important. So, when you would apply a contrast improving algorithm, you have to keep one thing in mind — do not overdo it. Histogram Equalization Histogram equalization is an image processing technique for adjusting the image’s intensity. This enhances the contrast in an image. Behind the scenes, it can be explained by using a histogram. An equalized histogram means that the image uses all gray levels in equal proportions. Then, intensities are better distributed on the histogram. The image below shows this visually with T being the transformation function. I think this should make sense for anyone with a statistical background, don’t you agree? Histogram Equalization (OpenCV documentation) OpenCV has a cv2.equalizeHist() method that performs a histogram on a grayscale input image. It has as output a histogram equalized image. This method is useful for images with both a bright/dark background and foreground. But, histogram equalization has an important limitation — it only works well when the distribution of the pixel values is similar throughout the image. When there are some regions that are significantly different (e.g. lighter or darker) from other parts of the image, the contrast in some regions might not be enhanced correctly. One example of such an image, light that enters from a window which makes some regions of the image significantly brighter. Adaptive Histogram Equalization To solve the shortcomings of the histogram equalization algorithm, you can compute multiple histograms for one image. Every histogram then corresponds to one specific region of the image. This causes every region of the image to be enhanced separately, thus solving the original problem. However, a new problem results from the adaptive histogram equalization algorithm. AHE can over-amplify the contrast in near-constant regions of the image. Since this over-amplification causes noise to be amplified in particular regions of the image, we need an alternative. I propose an alternative in the next section, so hang in there. Photo by Maxim Medvedev on Unsplash Contrast Limited AHE As discussed in the previous section, adaptive histogram equalization causes noise to be amplified in near-constant regions. Contrast limited AHE limits the contrast amplification to reduce amplified noise. It does so by distributing that part of the histogram that exceeds the clip limit equally across all histograms. But you want to see some results now, don’t you? I got something prepared, so let’s not keep you waiting any longer. The current president of the United States before and after applying CLAHE. Notice that the contours of the face are much more visual. You don’t have to follow me in this preprocessing step, but I think this is a visual improvement. The regions that matter most are slightly amplified. But how do you do this? Implementation With OpenCV I found that most implementations work only with grayscale images. So, I think it will be really useful if I would make a difference right here, by providing an example for a colored image. The implementation is as follows: CLAHE method for BRG images. I’ve only pasted the most important part, I figured that every seasoned data scientist/engineer should be capable of figuring out the rest. However, I’ll explain this method in a lot of detail. First of all, I converted the BGR image (blue, green, and red channel) to the HSV format (Hue, Saturation, and Value channel). This allows us to perform the CLAHE algorithm only on the value channel. We don’t want to mess up the hue or saturation of the image. Before I go further, let me explain HSV: Hue: The term for the pure spectrum colors commonly referred to by something called “color names” — red, orange, yellow, blue, green, and violet — which appear in the hue circle. Saturation: Refers to the intensity of color in an image. Primary colors — red, blue, and yellow are considered the purest as they are fully saturated. When saturation increases, colors are perceived as purer. Value: Refers to the lightness or darkness of a color. An image without hue or saturation is basically a grayscale image. Next, I apply CLAHE on the value channel of the image. I’ll explain the parameters I use as well. I found the default clipping limit of 40 way too aggressive and found generally improved results when using 2. The second parameter — the tilegrid size — splits an image into 64 tiles with 8 columns and 8 rows. Imagine that your input image is 160x160 pixels wide. In that case, I split the image into regions of 20x20 pixels. Both parameters are often used as default, so I didn’t bother changing them much. Keep in mind that if you want consistent behavior, every image should be on the same scale. In the last steps, I simply merge the changed value channel with the hue and saturation channel, before I return the image back to the original BGR format. When to Use It? Photo by Matt Artz on Unsplash I leave that to the reader to find out. For me personally, there are two phases where this could be useful. In the detection phase or the recognition phase. I’ll explain both and I will discuss alternatives if that is possible. Detection phase In this phase, it can certainly help to find more objects. I did the test and after using CLAHE, the model is able to detect more objects. It also has an application in, for example, MRI scans. For object detection, however, sometimes it is more useful to upsample your image. This has a better effect than using CLAHE on an image as a whole. Recognition phase As I showed, it can be certainly helpful to use CLAHE in the recognition phase. For example, when people need to classify images to train a model, it can visually aid them in performing that task better. It might also be beneficial for specific machine learning models as a preprocessing step. I’m not yet certain if it makes a life-changing difference for object recognition, because it’s hard to find good research on this topic. Feel free to comment on this story if you have some good research results, for example, in the domain of face recognition. Conclusion I hope to find some answers and hope that you, as a reader, can contribute to my search for improved object detection and recognition. I’m always open to a discussion on a topic I’m really enthusiastic about, so feel free to comment below. It is my intent to keep on learning about this subject. Towards Data Science A Medium publication sharing concepts, ideas, and codes. Follow 51 Thanks to Zack Shapiro.  Machine Learning Opencv Python Software Engineering Programming 51 claps Written by Dieter Jordens Follow Dieter is passionate about full stack agile software development and artificial intelligence. He is working as a Software Crafter at Continuum Consulting. Follow Towards Data Science Follow A Medium publication sharing concepts, ideas, and codes. Follow Write the first response More From Medium More from Better Programming More from Better Programming Should You Learn VIM as a Developer in 2020? Joey Colon in Better Programming Mar 19 · 4 min read 852 More from Better Programming More from Better Programming Fun Side Projects That You Can Build Today Daan in Better Programming Mar 11 · 6 min read 3K More from Better Programming More from Better Programming The Zero-Dollar Infrastructure Stack Tom Buyse in Better Programming Mar 20 · 5 min read 1K Discover MediumWelcome to a place where words matter. On Medium, smart voices and original ideas take center stage - with no ads in sight. Watch Make Medium yoursFollow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox. Explore Become a memberGet unlimited access to the best stories on Medium — and support writers while you’re at it. Just $5/month. Upgrade AboutHelpLegal
