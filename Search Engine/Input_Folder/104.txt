https://towardsdatascience.com/how-to-use-dbscan-effectively-ed212c02e62?source=collection_category---4------0-----------------------
How to Use DBSCAN Effectively - Towards Data Science Sign in Data Science Machine Learning Programming Visualization AI Video About Contribute How to Use DBSCAN Effectively A complete guide on using the most cited clustering algorithm effectively Vijini Mallawaarachchi Follow Mar 25 · 6 min read DBSCAN is an extremely powerful clustering algorithm. The acronym stands for Density-based Spatial Clustering of Applications with Noise. As the name suggests, the algorithm uses density to gather points in space to form clusters. The algorithm can be very fast once it is properly implemented. However, in this article, we would rather be talking about tuning the parameters of DBSCAN for a better utility than the algorithm implementation itself. (Implementation of DBSCAN is very simple. The harder part, if any, would be structuring data for neighbourhood lookups.) Before we start, make sure you have these packages in hand. numpy sklearn matplotlib # for visualization seabron # for pretty visualizations kneed # for our computations A Running Example Let us first create a set of data that could replicate a suitable set of data points for our analysis. Python is very generous with its set of libraries. For the purpose of data generation, we will be using sci-kit learn library’s make blobs function. from sklearn.datasets import make_blobs import seaborn as sns import matplotlib.pyplot as plt import numpy as npcenters = [[1, 0.5], [2, 2], [1, -1]] stds = [0.1, 0.4, 0.3] X, labels_true = make_blobs(n_samples=1000, centers=centers, cluster_std=stds, random_state=0)fig = plt.figure(figsize=(10, 10)) sns.scatterplot(X[:,0], X[:,1], hue=["cluster-{}".format(x) for x in labels_true]) Here, I have created 3 blobs of data. We can see the visualization of these data blobs as illustrated in Figure 1. For this example, I intentionally created 3 clusters with different densities to make clustering harder. Fig 1. Visualization of original clusters DBSCAN and its Parameters DBSCAN has a few parameters and out of them, two are crucial. First is the eps parameter, and the other one is min_points (min_samples). Latter refers to the number of neighbouring points required for a point to be considered as a dense region, or a valid cluster. Usually, we set this to a value that makes sense for the dataset and the number of dimensions present in the data. This will determine the number of outliers identified. However, this parameter is not as crucial as eps. Epsilon parameter of DBSCAN The most important parameter of DBSCAN can be identified as eps. It is the furthest distance at which a point will pick its neighbours. Therefore, intuitively this will decide how many neighbours a point will discover. Although for the min_points/min_samples we can give a default value, we cannot do so for eps. This will depend on the distribution of the data itself. Let us do DBSCAN with some guessed values for our dataset. The code and visualisation (in Figure 2) would be as shown below. db = DBSCAN(eps=0.5, min_samples=10).fit(X) labels = db.labels_fig = plt.figure(figsize=(10, 10)) sns.scatterplot(X[:,0], X[:,1], hue=["cluster-{}".format(x) for x in labels]) Fig 2. DBSCAN with eps=0.5 Tuning EPS parameter We can clearly see from our last figure, two clusters have been merged together. This is bad. Such situations can reduce recall in a real-world clustering application. Let’s try to vary eps and cluster again. The code and visualization (in Figure 3) would look like below. import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn.cluster import DBSCANfig = plt.figure(figsize=(20, 10)) fig.subplots_adjust(hspace=.5, wspace=.2) i = 1for x in range(10, 0, -1):     eps = 1/(11-x)     db = DBSCAN(eps=eps, min_samples=10).fit(X)     core_samples_mask = np.zeros_like(db.labels_, dtype=bool)     core_samples_mask[db.core_sample_indices_] = True     labels = db.labels_         print(eps)     ax = fig.add_subplot(2, 5, i)     ax.text(1, 4, "eps = {}".format(round(eps, 1)), fontsize=25, ha="center")     sns.scatterplot(X[:,0], X[:,1], hue=["cluster-{}".format(x) for x in labels])         i += 1 Fig 3. DBSCAN at varying eps values We can see that we hit a sweet spot between eps=0.1 and eps=0.3. eps values smaller than that have too much noise or outliers (shown in green colour). Note that in the image, I decrease eps by increasing my denominator in the code from 10 to 1. How can we do this automatically? A Systematic Method for Tuning the eps Value Since the eps figure is proportional to the expected number of neighbours discovered, we can use the nearest neighbours to reach a fair estimation for eps. Let us compute the nearest neighbours. from sklearn.neighbors import NearestNeighborsnearest_neighbors = NearestNeighbors(n_neighbors=11) neighbors = nearest_neighbors.fit(X) distances, indices = neighbors.kneighbors(X)distances = np.sort(distances[:,10], axis=0)fig = plt.figure(figsize=(5, 5)) plt.plot(distances) plt.xlabel("Points") plt.ylabel("Distance")plt.savefig("Distance_curve.png", dpi=300) Distance variation at the 10th neighbour Note that in the nearest neighbour calculation, the point itself will appear as the first nearest neighbour. So we seek the 11 nearest neighbours. We sort the distance to the 10th nearest neighbour and plot the distance variation. As we can see, the elbow point appears somewhere in between 0.1 and 0.3. Quite what we were expecting isn’t it? I choose the 10th neighbour considering the fact that I pick 10 as min_samples value for clustering. I hope it makes sense so far. KneeLocator to Detect Elbow Point Ville Satopaa et al. presented the paper “Finding a “Kneedle” in a Haystack: Detecting Knee Points in System Behavior” in the year 2011. In this article, for the purpose of detecting the elbow point (or knee point), I will be using their python library kneed. We can use the following code to find and plot the knee point. i = np.arange(len(distances)) knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')fig = plt.figure(figsize=(5, 5))knee.plot_knee() plt.xlabel("Points") plt.ylabel("Distance") print(distances[knee.knee]) The plot of Knee Point We can see that the detected knee point by this method is at distance 0.178. Now we can use this value as our eps to see how our new clustering would look like. DBSCAN with Auto-detected Eps We can see that we have a reasonable estimate of the actual clustering. This is usually good enough for research work. If non-existence of out-liers is an intuitive assumption for the scenario, one can simply use the computed nearest neighbours to re-assign the outlier points (named as cluster--1) to detected clusters. Limitations There are a few implicit assumptions in this approach. Densities across all the clusters are the same. Cluster sizes or standard deviations are the same. These assumptions are implied when we consider the same neighbour level for knee computation. However, in the original data, we can clearly see that the densities are not the same. This is the main reason why we observe a few outliers even though the points are distributed using a fixed standard deviation when we create blobs. Moreover, fixing these is beyond the scope of this article. Final Thoughts I have attached a jupyter notebook with the complete code used for the examples in this article. You can access the notebook from the link below. I hope you enjoy reading my article as much as I did so writing. Feel free to try these out in your research work. It helped me a lot. Thanks for reading! Cheers! ? Towards Data Science A Medium publication sharing concepts, ideas, and codes. Follow 118 Machine Learning Data Science Programming Computer Science Algorithms 118 claps Written by Vijini Mallawaarachchi Follow PhD Student at Australian National University | Loves Bioinformatics, Data Science, Music & Astronomy Follow Towards Data Science Follow A Medium publication sharing concepts, ideas, and codes. Follow Write the first response More From Medium More from Towards Data Science More from Towards Data Science from sklearn import * Conor Lazarou in Towards Data Science Mar 22 · 9 min read 2.5K More from Towards Data Science More from Towards Data Science Top 3 Python Functions You Don’t Know About (Probably) Dario Rade?i? in Towards Data Science Mar 14 · 4 min read 4.3K More from Towards Data Science More from Towards Data Science Don’t learn machine learning Caleb Kaiser in Towards Data Science Mar 19 · 4 min read 2.4K Discover MediumWelcome to a place where words matter. On Medium, smart voices and original ideas take center stage - with no ads in sight. Watch Make Medium yoursFollow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox. Explore Become a memberGet unlimited access to the best stories on Medium — and support writers while you’re at it. Just $5/month. Upgrade AboutHelpLegal
