https://towardsdatascience.com/label-propagation-demystified-cd5390f27472?source=user_profile---------8-----------------------#--responses
Label Propagation Demystified - Towards Data Science Sign in Data Science Machine Learning Programming Visualization AI Video About Contribute Label Propagation Demystified A simple introduction to graph-based label propagation Vijini Mallawaarachchi Follow Mar 6 · 7 min read Social media networks have spanned across the globe and are growing day-by-day. Consider a social media network where you know the interests of some people and you want to predict the interests of others so that we can target marketing campaigns. For this purpose, we can use the graph-based semi-supervised machine learning technique called Label Propagation. In this article, I will explain the label propagation process with some examples and sample code. What is Label Propagation? Label Propagation Algorithm (LPA) is an iterative algorithm where we assign labels to unlabelled points by propagating labels through the dataset. This algorithm was first proposed by Xiaojin Zhu and Zoubin Ghahramani [1] in the year 2002. LPA comes under transductive learning, as we want to predict labels of the unlabelled data points which are already given to us. Assume that we have a network of people as given below with two label classes “interested in cricket” and “not interested in cricket”. So the question is, can we predict whether the remaining people are interested in cricket or not? Original image by Gordon Johnson from Pixabay. For LPA to work in this case, we have to make an assumption; an edge connecting two nodes carry a notion of similarity. i.e. if two people are connected together, that means that it is highly likely that these two people share the same interests. We can make this assumption as people tend to connect with other people having similar interests. Walking Randomly in the Graph Consider the sample graph given in Figure 1, where we have 2 label classes (red and green) and 4 nodes coloured (2 for each class). We want to predict the label of node 4. Fig 1. Sample graph 1 We can walk randomly in the graph, starting from node 4 until we meet any labelled node. When we hit a labelled node, we stop the walk. Hence, these labelled nodes are known as absorbing states. Let’s consider all the possible walks from node 4. Out of all the possible walk, the following walks will end in a green node. 4 ? 9 ? 15 ? 16 4 ? 9 ? 13 ? 14 4 ? 9 ? 13 ? 15 ? 16 4 ? 9 ? 15 ? 13 ? 14 The following walks will end in a red node. 4 ? 7 ? 8 4 ? 7 ? 6 ? 5 ? 1 4 ? 5 ? 1 4 ? 5 ? 6 ? 7 ? 8 4 ? 2 ? 1 Based on all the possible random walks starting from node 4, we can see that the majority of the walks end in a red node. So, we can colour node 4 in red. This is the basic intuition behind LPA. Mathematical Formulation Let X? be the set of labelled nodes and Y? be the one-hot labels of the labelled data (If you don’t know about one-hot encoding, you can refer this link). Assume that there are {1,…,C} class labels. X? are unlabelled vertices. We do not know Y? and hence Y? will contain zeros. We can express the random walks as given below. Fig 2. Random walks In matrix form, the equation will look as given below. Fig 3. Random walks in matrix form If we can compute the probabilistic transition matrix T, we can compute all the label probabilities of the unlabelled nodes. How to Compute the Probabilistic Transition Matrix? Fig 4. Sample graph 2 Consider the sample graph with absorbing states as shown in Figure 4. For each node, we have to calculate the probability of jumping to other nodes. When we reach absorbing states, the walk ends as we get trapped in the absorbing state (represented as a self-loop in the graph). This is an undirected graph, so we can move in any direction. Assuming that the probability of transitioning from a node to its neighbours is equally probable, we can write T as below. Fig 5. Matrix T for sample graph 2 found in Figure 4 The probability to get from node 1 to node 1 is 1 as node 1 is an absorbing state. From node 1, we cannot reach any other node. Hence the probabilities of reaching other nodes from node 1 will be 0. The same way goes to node 2. From node 4, you can go to nodes 1, 3 and 5. Hence it is equally probable to move from node 4 to nodes 1, 3 and 5 with a probability of 0.33 for each node. Similarly, from node 5, we can move to nodes 4 and 6 with a probability of 0.5 for each node. Note that we can calculate T using the degree matrix (D) and the adjacency matrix (A) of the graph using the following equation. T = D?¹A Now notice that we can split the matrix T as shown in Figure 6. Fig 6. T can be broken into 4 blocks T?? — Probability to get from labelled nodes to labelled nodes T?? — Probability to get from labelled nodes to unlabelled nodes T?? — Probability to get from unlabelled nodes to labelled nodes T?? — Probability to get from unlabelled nodes to unlabelled nodes Note: T?? will be an identity matrix and T?? will be a zero matrix because we cannot move out from labelled nodes as they are absorbing states. What will happen if we multiply matrix T for t times by itself and then send t to infinity (?)? You can type this matrix in MATLAB and get T¹??. You will get a result like this. Fig 7. T multiplied by itself for 100 times on MATLAB When you raise T to a bigger power, the probabilities will stop changing (get saturated) and result in steady transition probabilities. You can see now that only the first two columns contain non-zero values and the rest have zeros. We can mathematically depict this as follows. Fig 7. The formula for T multiplied by itself for infinite times Obtaining the Final Answer Finally, the matrix with labels will look like this where we can get the label vectors of labelled nodes and label vectors of unlabelled nodes. Fig 8. The formulas for one-hot labels of labelled and unlabelled nodes Now let’s consider the sample graph 2 in Figure 4 where we want to predict the labels of the unlabelled nodes. Using our MATLAB results we can get the labels as follows. Fig 9. Obtaining labels of unlabelled nodes For each unlabelled node, we assign the class label which has the maximum probability. However, you can see that node 5 has equal probability to be red and green. Accordingly, our final labelled graph will be as shown in Figure 10. Fig 10. Final labelling of the sample graph 2 Sample Code Sample code with a simple implementation of the LPA You can also read my previous article Visualising Graph Data with Python-igraph to learn how to represent graph data in Python. Visualising Graph Data with Python-igraph An introduction to python-igraph module using the CiteSeer dataset towardsdatascience.com Final Thoughts LPA uses the labels of the already labelled nodes as a base and tries to predict the labels of the unlabelled nodes. However, if the initial labelling is wrong, this can affect the label propagation process and erroneous labels may get propagated. To overcome this issue, label spreading was introduced, where we learn the labels of the labelled nodes as well while learning the labels of the unlabelled nodes. This sort of applies some label correction as well. You can read more about label spreading from the article titled Learning with Local and Global Consistency by Dengyong Zhou et al. Hope you found this explanation useful. I would love to hear your thoughts. Thank you for reading. Cheers! References [1] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, Technical Report. CMU-CALD-02–107, 2002. [2] Network Analysis. Lecture 17 (part 1). Label propagation on graphs by Prof. Leonid Zhukov (https://youtu.be/hmashUPJwSQ). Towards Data Science A Medium publication sharing concepts, ideas, and codes. Follow 13 Machine Learning Computer Science Graph Mathematics Data Science 13 claps Written by Vijini Mallawaarachchi Follow PhD Student at Australian National University | Loves Bioinformatics, Data Science, Music & Astronomy Follow Towards Data Science Follow A Medium publication sharing concepts, ideas, and codes. Follow See responses (1) More From Medium More from Towards Data Science More from Towards Data Science from sklearn import * Conor Lazarou in Towards Data Science Mar 22 · 9 min read 2.5K More from Towards Data Science More from Towards Data Science Top 3 Python Functions You Don’t Know About (Probably) Dario Rade?i? in Towards Data Science Mar 14 · 4 min read 4.3K More from Towards Data Science More from Towards Data Science Don’t learn machine learning Caleb Kaiser in Towards Data Science Mar 19 · 4 min read 2.4K Discover MediumWelcome to a place where words matter. On Medium, smart voices and original ideas take center stage - with no ads in sight. Watch Make Medium yoursFollow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox. Explore Become a memberGet unlimited access to the best stories on Medium — and support writers while you’re at it. Just $5/month. Upgrade AboutHelpLegal
