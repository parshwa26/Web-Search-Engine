https://towardsdatascience.com/building-a-data-science-development-environment-with-docker-compose-1407aa5147b9?source=user_profile---------6-----------------------
Building a Data Science Development Environment With Docker Compose Sign in Data Science Machine Learning Programming Visualization AI Video About Contribute Building a Data Science Development Environment With Docker Compose Learn Docker, Docker Compose, and Cookiecutter for project management Lora Johns Follow Oct 1, 2019 · 6 min read When you have a great new idea, the last thing you want is to set aside the data and make a clean workspace. Each of us has indulged in just doing “one quick thing” in the IDE without thinking of best practices for dependency management and reproducibility. If we’re honest with ourselves, do we even know what “best practices” are? Classic. Virtual environments aren’t easy, whatever tutorials may say. Sometimes, I’ve spent more time getting my development environment set up correctly (or fixing what’s gone wrong) than working with data and models. Anaconda frequently causes problems, particularly with R and NLP libraries (but that deserves its own post). I’ve also used Homebrew, conda, pip, venv, pyenv, virtualenv and virtualenvwrapper, pyenv-virtualenv and pyenv-virtualenvwrapper, and other tools to manage packages and Python versions. (I’ll write another post detailing the myriad tools that exist and how you can put them in a real workflow.) If you want them to work properly, they require you to pay close attention to how your operating system interacts with the software, your user permissions, your path variables, and your shell configuration files. Even despite a pristine setup, I still encountered package conflicts, $PATH issues, and various other cryptic failures to function. The silver lining is that I learned a lot about UNIX. Docker lets you develop code and interact with data in an isolated environment. Docker packages all your project’s code and dependencies in an executable container that can be saved and reused. A container image is standalone software that includes everything you need to recreate the original environment: code, runtime, system tools, system libraries, and settings. You can share images via Dockerhub; if you write your Dockerfile well, your project will run the same way every time, on any machine. It’s still not foolproof, but since it’s self-contained, there are fewer opportunities for adverse interactions, and if you screw up, you can kill the container and restart it. Docker Compose lets you run multi-container images (like a database server, an IDE, and a web app that all work together). While there may arguably be tools better optimized for containerization per se, Docker scales easily and quickly, can parallelize applications, and is widely used for development/testing/deployment and continuous-integration workflows. It’s no surprise that every working data scientist I’ve met regards Docker as indispensable. To learn how to use Docker and Docker Compose (and to make something useful for myself), I made a cookiecutter for creating virtual environments in Python for natural language processing projects. (You can pull the repo yourself and use it to create, among other things, Python scripts or Jupyter notebooks using pandas, numpy, seaborn, and various NLP libraries, if you like.) Essentially, it’s a customizable, editable project template that creates a directory on your computer, writes the Dockerfile, docker-compose file, and requirements.txt for python packages, and creates a Makefile that lets you quickly build and interact with your new Docker image. (Caveat: It’s still a work in progress.) You will certainly learn a few lessons in creating a Python package to teach yourself Docker. Here’s a sampling: Lesson 1: Don’t necessarily trust tutorials. Always double-check any tutorial or resource you come across. Nearly all of the time, they are out-of-date. Rapid iteration and little to no information verification of online publication means you must read all the way through before you start to implement any code. And almost all of them have code that’s wrong for you. Some Dockerfiles out there are bad practice; others have syntax that’s out of date or won’t run; and beware the irony of the ones that are built on out-of-date versions of Python or Ubuntu when they really should be using a different build. (And the “latest” tag doesn’t mean what you think it does.) You can’t successfully build an image—or get the result that you intend—unless the syntax of your docker-compose.yml is precise and the composition of your Dockerfile is deliberate. You’re almost certainly better off writing it yourself than copying and pasting from a gist or tutorial. Just because it’s on the internet does not mean it’s valid. Write the file. Set up tests and test your code. Lesson 2: Your project will take longer than you think. Getting quality information takes time. Docker has good documentation, but it’s not always well-indexed or up-to-date. (I found the answer to one bug in my docker-compose file in a GitHub issue, in which a Docker developer replied, in essence: “Oops—yeah, we changed that syntax and forgot to update the documentation.”) These tools take more effort to acquire than a simple “Hello World” tutorial or blog post may imply. Once you get past the toy example and want to use the code you’re writing to do real work, it gets multifaceted quickly. For my project, I had to be very comfortable with computer networking, Linux/Ubuntu, bash scripting, GNU make, reading and editing other people’s source code, git and GitHub, and the cookiecutter package. Add in Docker and Docker Compose, Python, and all of the images you want to include in your environment (e.g., Redis, Flask, Django, Nvidia) and you’re looking at a lot of interlocking parts. It can be overwhelming. Getting lost on a complicated tangent is frustrating. It’s hard to keep it simple, but don’t try to draw the whole owl at once. Get something minimal working. I got the cookiecutter to work, then the Makefile, then the Docker image for Python, then the docker-compose file, then the whole thing all together. And it still is far from perfect. I have weaknesses to fix, more tests to write, better structures to implement… and on and on. Still, I learned a lot by building the guts of this project myself, and I’ll learn more by improving it piece by piece. Lesson 3: You can’t “just do data science” Anyone who has job searched outside of academia knows this already, but for those who haven’t, it’s worth noting: your skills need to demonstrate more than just “pure” data science if you’re in the market for a “data” job. Job titles do not map cleanly to careers or even duties, and the market evolves too rapidly to make it a safe bet. Job descriptions for “data science” or “machine learning” with “scientist” or “engineer” attached usually don’t just describe a candidate who’s good with finding, acquiring, cleaning, visualizing, and analyzing data; they also want someone who knows cutting-edge machine-learning and classical statistical models, distributed computing, NoSQL databases, algorithms, advanced mathematics, software development and testing, DevOps, and a healthy dose of domain knowledge as well. You may rather be building random forests and web-scraping cool data, but if you are a job-seeker, it’s worth it to spend some time learning the less sexy tools of industry. Understanding what your colleagues are using and being able to speak to the pros, cons, advantages and pitfalls—not to mention implement those tools—makes you a better collaborator and, therefore, more effective at your job, because very few data professionals work in total isolation. Give it a try, one step at a time. Download Docker. Pull a simple image to get used to Docker’s basic workflow. (If you don’t mind how rudimentary it is, you can pull my repo—there’s a README, and will be a tutorial, which of course you can’t trust, TBA soon.) Use repo2docker to convert your favorite GitHub repo into a container, and run it. Take the Dockerfile apart to see how it turns base code into Docker images. To learn Docker, make it work for you. If you want to understand it, work on a project that interests you, whether for its technical complexity or subject matter. Get frustrated by the state of bad information on the web. Build, test, break, repeat your code for longer than you think you need to. Then, make an app that functions—not like a piece of million-dollar software, but basically does the job. Reflect on all you’ve learned; be reasonably satisfied. And then, later, after a joyride on the built-in seaborn datasets inside the new container’s IDE, you can think about adding more features and tightening up your tests. But maybe not tonight. Towards Data Science A Medium publication sharing concepts, ideas, and codes. Follow 355 Data Science Programming Docker Python Software Development 355 claps Written by Lora Johns Follow Machine learning, linguistics, NLP | { B.A. : Dartmouth, J.D. : Yale, M.S. : Simmons } Follow Towards Data Science Follow A Medium publication sharing concepts, ideas, and codes. Follow Write the first response More From Medium More from Towards Data Science More from Towards Data Science from sklearn import * Conor Lazarou in Towards Data Science Mar 22 · 9 min read 2.5K More from Towards Data Science More from Towards Data Science Top 3 Python Functions You Don’t Know About (Probably) Dario Rade?i? in Towards Data Science Mar 14 · 4 min read 4.3K More from Towards Data Science More from Towards Data Science Don’t learn machine learning Caleb Kaiser in Towards Data Science Mar 19 · 4 min read 2.4K Discover MediumWelcome to a place where words matter. On Medium, smart voices and original ideas take center stage - with no ads in sight. Watch Make Medium yoursFollow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox. Explore Become a memberGet unlimited access to the best stories on Medium — and support writers while you’re at it. Just $5/month. Upgrade AboutHelpLegal
