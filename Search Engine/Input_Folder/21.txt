https://towardsdatascience.com/a-visual-intuition-for-regularization-in-deep-learning-fe904987abbb?source=collection_home---4------2-----------------------
A Visual Intuition For Regularization in Deep Learning Sign in Data Science Machine Learning Programming Visualization AI Video About Contribute A Visual Intuition For Regularization in Deep Learning What happens to our model as we apply regularization? Kelvin Lee Follow Mar 24 · 11 min read The strength and weakness of deep learning models—freedom to approximate any function, but also freedom to approximate any other function other than the one you’re interested in! Gaining an Intuition for Regularization In machine learning, regularization is a approach used to combat high variance — in other words, the issue of your model learning to reproduce the data, rather than the underlying semantics about your problem. In an analogous way to humans learning, the idea is to construct your homework problems to test and build for knowledge, rather than simply rote learning: for example, learning multiplication tables as opposed to learning how to multiply. This kind of phenomenon is especially prevalent in learning by neural networks — with great learning capacity comes a large likelihood for memorization, and it is up to us practitioners to guide deep learning models into soaking up our problem, not our data. Many of you will have come across these methods in the past, and may have developed your own intuition for how different regularization methods affect the outcome. For those of you who don’t (and even for those who do!) this article provides a visual guide for how neural network parameters are shaped by regularization. It’s important to visualize these aspects, as it’s extremely easy to take many concepts for granted; the figures and their explanations in this article will hopefully help you create an intuition for what really happens to your model parameters as you increase regularization. In this article, I’ll cover L2 and dropouts as standard forms of regularization. I won’t discuss how other methods (such as collecting more data) might change the way your model works; that’s probably for another time. All of the figures and models were made with the standard scientific Python stack: numpy, matplotlib, scipy, sklearn, and the neural network models were built with PyTorch. Developing a complex function One of the core tenets for deep learning is the ability for deep neural networks to act as universal function approximations. The idea that whatever you may be interested in—spread of disease, self-driving cars, astronomy—can be compressed and expressed by a self-learning model is absolutely amazing! This is despite whether or not the problem you are interested in can actually be expressed as some analytic function f. As you condition a machine learning model through training, the model takes on parameters ? that allows the model to approximately learn f*. For illustration purposes, we’re going to be looking at some relatively simple data: ideally, something in one-dimension that is complex enough for old-school curve fitting to be a pain, but not hard enough to make abstraction and understanding for us to be difficult. So, I’m going to create some arbitrarily complex function that mimics periodic signals, but with some funkiness added to it. The function below implements the following equation: Our very own “complex” function where A, B, C are random numbers sampled from different Gaussian distributions. The effect of each of these values is to add lag between very similar functions, such that they add together randomly to generate very different values of f. We will also add white (Gaussian) noise to the data, to simulate the effect of the data being collected. Let’s visualize a randomly generated sample of this data: for the rest of this article, we’ll be looking to reproduce this curve with a small neural network. Visualization of our “complex” function. This corresponds to an X and Y 1D array, each 10,000 elements long. To do our model training, we’re going to have split up this into training/validation sets. For this, I’ll be using the extremely convenient train_test_split function in sklearn.model_selection. Let’s plot up the training and validation sets: Blue points correspond to testing, red represents training data. As we see in the plot, both sets do a pretty good job in representing the overall curve: if we were to remove one or the other, we could glean more or less the same picture of what the data represents. This is a pretty vital aspect to cross-validation! Developing our model Now that we have a data set, we’re going to need a relatively simple model to try and reproduce it. For this purpose, we’re just going to be dealing with a four-layer neural network, comprising single input and output values with three hidden layers, each 64 neurons wide. Simplified picture of our neural network model: singular value input and output, with three hidden layers 64 neurons wide each (not all neurons are drawn!) For convenience, each hidden layer has a LeakyReLU activation, with ReLU activation on the output. In principle these shouldn’t matter so much, but during testing sometimes the model had trouble learning some of the “complex” functions, particularly when activations like tanh and sigmoid were used, which saturated easily. For the purposes of this article, the specifics of this model doesn’t matter much: all that matters is that it’s a fully-connected neural network that has some capacity to learn to approximate some function. Just to demonstrate that the model works, I performed the usual training/validation cycles using the mean-squared-error (MSE) loss and the ADAM optimizer, without any form of regularization and ended up with these results: Training and validation for this model works well: no real evidence of high variance or bias and both losses decrease relatively monotonically over the course of 100 epochs. When we use this model to predict the function: Our model more or less gets the approximate function perfectly! Apart from regions of rapidly changing curvature (near x=11) the model reproduces our “complex” function pretty well! Now, I can hear you asking: why am I doing any regularization if the model works well? For the purposes of this demonstration, it doesn’t matter so much whether or not our model is overfitting: what I want to get across is how regularization affects a model; in our case, how it can even detrimentally affect a perfectly working model. In some sense, you can interpret this as a word of warning: deal with overfitting as you encounter it, but not before. In the wise words of Donald Knuth, “Premature optimization is the root of all evil”. How regularization affects your parameters Now that we’ve got all the boilerplate stuff out of the way, we can get to the meat of the article! Our focus is to try and develop an intuition for how different ways of regularization can affect our simple model from three perspectives: What happens to the training/validation loss? What happens to our model performance? What happens to the actual parameters? While the first two points are somewhat straightforward, many of you may not be familiar on how the third point can be quantified. In this demonstration, I’ll be using kernel density estimation to measure the spread/variation in parameter values: for those who are familiar with Tensorboard, you’ll have seen these plots; for those who aren’t, think of these plots as sophisticated histograms. The goal is to visualize how our model parameters change with regularization, and the plot below shows the difference in the distributions of ? before and after training: Kernel density estimates for model parameters ? for our simple model. The blue curve is labelled “uniform”, because it represents our model parameters initialized with a uniform distribution: you can see how this manifests as basically a top-hat function, with equal probability across the center. This contrasts with the trained model parameters: after training, the model requires values of ? that are non-uniform in order to actually express our function. L2 regularization One of the most straightforward approaches to regularization is the so called L2 regularization: the L2 refers to the fact that the L2 norm of our parameter matrices are used. From linear algebra, the norm of a matrix is given by: General expression for arbitrary Lpq norms In pre-neural network machine learning, where parameters are more often expressed as vectors rather than matrices/tensors, this is simply the Euclidean norm. In deep learning, we’re more commonly dealing with matrices/high dimensional tensors, and the Euclidean norm doesn’t extend very well (beyond Euclidean geometry). The L2 norm is actually a special case of the equation above, where p=q=2 and is referred to as the Frobenius or Hilbert-Schmidt norm, which generalizes to infinite dimensionality (i.e. Hilbert space). The Frobenius/Hilbert-Schmidt (L2) norm. In deep learning applications, the general form of applying this L2 regularization is to append a “penalty” factor at the end of your cost function J: Our cost function: the first part is the mean-squared-error difference between predicted and actual values, and the part on the right is our L2 regularization term. Very simply, this equation defines the cost function J as the MSE loss, as well as the L2 norm. The effect of the L2 norm has on the cost is multiplied by this prefactor ?; this is referred to in many implementations as a “weight decay” hyperparameter, usually between 0 and 1. Since it controls the amount of regularization, we need to understand what this does to our model! In a series of experiments, we’re going to repeat the same training/validation/visualization cycle as we did before, however on a range of values of ?. First, how does it affect our training? The affect of the L2 regularization on model training. Let’s break down the plot above. The deeper shades of red correspond to larger values of ? (although it’s not a linear map!), showing traces of the training loss as the log of the MSE loss. Remember in our un-regularized model, these curves decreased monotonically. Here, as we increase the value of ? the ultimate training error is substantially increased, and the decrease in loss at early epochs are also not as dramatic. What happens when we try to use these models to predict our function? Predictions using models trained with specified values of ?. We can see that, with small values of ? the function can still be expressed reasonably well. The turning point appears to be around ?=0.01, where the qualitative shape of the curve is reproduced but not the actual data points. From ?>0.01, the model just predicts what appears to be the mean of the whole data set: as if we simply tried to do linear regression. If we interpret these with respect to our training loss, it’s not wonder that the loss stops decreasing—there’s only so much you can do with a straight line! What about the distribution of parameters? Distribution of parameters for each of our trained models, with various values of ?. The peaks of the distributions are cut off. We see that the spread of parameter values is substantially hampered, as we go from low to high ?. Compared to the uniform distribution, the spread of parameter values shrinks closer and closer to zero, and with ?=1.0 the distribution of ? just looks like a Dirac delta function at zero. From this, we can take away that L2 regularization acts to constrain parameter space—forcing ? to be very sparse and close to zero. What about dropouts? Another popular and cost-efficient way of regularization is to incorporate dropouts in your model. The idea is that with each model pass, a number of neurons are deactivated by setting their weights to zero according to some probability p. In other words, we apply a boolean mask to our parameters, and each time data passes through different units are activated. The rationale behind this is to distribute the model learning throughout the network, as opposed to specifically one or two layers/neurons. For our experiments, we’re going to include dropout layers between each hidden layer, and adjust the dropout probability p from zero to one. In the former limit, we should just have an un-regularized model while in the latter we should have severally decreased learning capacity, as every hidden layer is deactivated. Model training losses for a range of dropout probabilities. We see a very similar effect to the L2 regularization: overall, the learning capacity of the model is decreased and with larger values of dropout probability the ultimate loss is proportionally larger. When we try to use these models to predict our function: Going down the page, we increase in the dropout probability. Starting from p=0.1, we can see that our model starts being quite shaky about its prediction: the most interesting thing is that it appears to approximately trace our data, including the noise! At p=0.2 and 0.3, this is even more obvious around x=11—recall that our un-regularized model had difficulty getting this area of the function right. We see that the predictions with dropouts actually makes this region incredibly fuzzy, which is almost like the model telling us that it’s uncertain! (More on this later). From p=0.4 onwards, it seems like the capacity of the model is sufficiently hampered that it fails to reproduce most of the curve with the exception of the first portion. At p=0.6, it looks like the predictions nearly approximate the data set mean, which was what seemed to also happen to large values of L2 regularization. What about our model parameters? Distribution of model parameters as a function of dropout probability. Compare this with our the L2 norm results: with dropouts, our distribution of parameters is much wider, which increases the ability of our model to express. With the exception of p=1.0, it does not appear that the actual value of dropout probability affects the distribution of parameters much, if at all. At p=1.0, our model fails to learn anything, and just resembles the uniform distribution. At a reduced value of p, the model can still manage to learn albeit at a reduced rate. Take-home messages From our simple experiments, I hope you have developed some mental model of how these two regularization methods affect neural network models, from the three perspectives we explored. L2 regularization is as straightforward as it gets, with a single hyperparameter to tune. As we increase the weighting of the L2 penalty, the variation in parameter space—and therefore model capacity—decreases extremely quickly for large values (0.01–1). With smaller values, you may not even see a difference in your model predictions, although it becomes apparent when you plot out the distributions of ?. Dropouts is a more sophisticated method for regularization, as you now have to deal with another layer of hyperparameter complexity (p can have different values for different layers). Despite this, depending on how you look at it, this can actually provide another dimension of model expression: in the form of model uncertainty. I plan to cover this more in another blog post (send me a message to encourage me!). The effect of including dropouts is that the variation in ? becomes significantly larger and spread out over different possible values of ?. In both methods, we’ve seen that regularization increases the ultimate training loss. The cost of these artificial forms of regularization (as opposed to getting more training data) is that they can decrease the capacity of your model: it’s not something you want to include unless you are certain that your model needs regularization. With this guide, however, you should now know how either form affects your model! If you’re interested, you can run some of the code on Binder. I wouldn’t necessarily run the torch models (it’ll exhaust their poor resources), but you can use it to explore the code in a notebook. Further reading Dropout regularization My github repo with the experiments Please look out for more of this kind of article! You can reach out to me on Twitter, LinkedIn, and of course on the Medium network! Towards Data Science A Medium publication sharing concepts, ideas, and codes. Follow 58 Deep Learning Regularization Machine Learning Visualization 58 claps Written by Kelvin Lee Follow Astrochemistry researcher at the Center for Astrophysics | Harvard & Smithsonian. Obsessed with automated workflows, machine learning, and inference. Follow Towards Data Science Follow A Medium publication sharing concepts, ideas, and codes. Follow See responses (1) More From Medium More from Towards Data Science More from Towards Data Science from sklearn import * Conor Lazarou in Towards Data Science Mar 22 · 9 min read 2.5K More from Towards Data Science More from Towards Data Science Top 3 Python Functions You Don’t Know About (Probably) Dario Rade?i? in Towards Data Science Mar 14 · 4 min read 4.3K More from Towards Data Science More from Towards Data Science Don’t learn machine learning Caleb Kaiser in Towards Data Science Mar 19 · 4 min read 2.4K Discover MediumWelcome to a place where words matter. On Medium, smart voices and original ideas take center stage - with no ads in sight. Watch Make Medium yoursFollow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox. Explore Become a memberGet unlimited access to the best stories on Medium — and support writers while you’re at it. Just $5/month. Upgrade AboutHelpLegal
